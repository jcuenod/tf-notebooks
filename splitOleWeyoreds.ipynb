{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Find Split Ole Weyored\n",
    "\n",
    "The objective of this notebook is to find the ole weyored accent -- a composite accent which only occurs in the poetic accentuation system. Often it occurs on single words but, because it is a composite accent, sometimes it occurs split over two words.\n",
    "\n",
    "We begin by importint the necessary text-fabric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.3.2\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "114 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.18s B g_word_utf8          from /home/jcuenod/Programming/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.07s B trailer_utf8         from /home/jcuenod/Programming/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s Feature overview: 108 nodes; 5 edges; 1 configs; 7 computeds\n",
      "  4.01s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "from tf.fabric import Fabric\n",
    "\n",
    "TF = Fabric(locations='../text-fabric-data', modules='hebrew/etcbc4c')\n",
    "api = TF.load('trailer_utf8 g_word_utf8')\n",
    "\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we process all nodes looking for the poetic ones..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we should have a list of nodes from poetic passages:\n",
      "--\n",
      "poetic_passage_word_nodes 45142\n",
      "poetic_passage_word_nodes_blacklist 1060\n",
      "reduced_list 44082\n"
     ]
    }
   ],
   "source": [
    "poetic_passages = [\n",
    "    (\"Psalms\",),\n",
    "    (\"Proverbs\",),\n",
    "    (\"Job\",),\n",
    "]\n",
    "poetic_passages_blacklist = [\n",
    "    (\"Job\", 1),\n",
    "    (\"Job\", 2),\n",
    "    (\"Job\", 3, 1),\n",
    "    (\"Job\", 42, 7),\n",
    "    (\"Job\", 42, 8),\n",
    "    (\"Job\", 42, 9),\n",
    "    (\"Job\", 42, 11),\n",
    "    (\"Job\", 42, 12),\n",
    "    (\"Job\", 42, 13),\n",
    "    (\"Job\", 42, 14),\n",
    "    (\"Job\", 42, 15),\n",
    "    (\"Job\", 42, 16),\n",
    "    (\"Job\", 42, 17),\n",
    "]\n",
    "\n",
    "from itertools import chain\n",
    "def createWordNodeListFromPassageTupleList(passage_list):\n",
    "    cumulative_node_list = []\n",
    "    for passage in passage_list:\n",
    "        filter_node = T.nodeFromSection(passage)\n",
    "        if filter_node is not None:\n",
    "            cumulative_node_list = list(chain(cumulative_node_list, L.d(filter_node, otype='word')))\n",
    "        else:\n",
    "            print(\"Failed on\", passage)\n",
    "    return cumulative_node_list\n",
    "\n",
    "poetic_passage_word_nodes = createWordNodeListFromPassageTupleList(poetic_passages)\n",
    "poetic_passage_word_nodes_blacklist = createWordNodeListFromPassageTupleList(poetic_passages_blacklist)\n",
    "\n",
    "reduced_list = list(filter(lambda x: x not in poetic_passage_word_nodes_blacklist, poetic_passage_word_nodes))\n",
    "\n",
    "print(\"Now we should have a list of nodes from poetic passages:\")\n",
    "print(\"--\")\n",
    "print(\"poetic_passage_word_nodes\", len(poetic_passage_word_nodes))\n",
    "print(\"poetic_passage_word_nodes_blacklist\", len(poetic_passage_word_nodes_blacklist))\n",
    "print(\"reduced_list\", len(reduced_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding accent units:\n",
      "Found: 29649\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinding accent units:\")\n",
    "accent_units = []\n",
    "glue = {'', '־'}\n",
    "node2au = []\n",
    "current_au = \"\"\n",
    "current_au_nodes = []\n",
    "for w in reduced_list:\n",
    "    trailer = F.trailer_utf8.v(w)\n",
    "    current_au += F.g_word_utf8.v(w) + trailer\n",
    "    current_au_nodes.append(w)\n",
    "    if trailer not in glue:\n",
    "        accent_units.append({\n",
    "            \"accent_unit\": current_au,\n",
    "            \"nodes\": current_au_nodes\n",
    "        })\n",
    "        current_au = \"\"\n",
    "        current_au_nodes = []\n",
    "print(\"Found:\", len(accent_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "unicode_accent_range = '[\\u0591-\\u05AE\\u05BE\\u05C0\\u05BD\\u05C3]'\n",
    "ole_accent = \"\\u05AB\"\n",
    "yored_accent = \"\\u05A5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the Ole Weyoreds that are found on single words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Psalms', 1, 1) רְשָׁ֫עִ֥ים \n",
      "('Psalms', 1, 2) חֶ֫פְצֹ֥ו \n",
      "('Psalms', 3, 3) לְנַ֫פְשִׁ֥י \n",
      "('Psalms', 4, 9) וְאִ֫ישָׁ֥ן \n",
      "('Psalms', 5, 7) כָ֫זָ֥ב \n",
      "('Psalms', 5, 10) הַ֫וֹּ֥ות \n",
      "('Psalms', 5, 13) צַ֫דִּ֥יק \n",
      "('Psalms', 7, 1) לְדָ֫וִ֥ד \n",
      "('Psalms', 7, 9) עַ֫מִּ֥ים \n",
      "(only printing the first 10)\n",
      "Found altogether: 265\n"
     ]
    }
   ],
   "source": [
    "ole_weyored = [\"\\u05AB\", \"\\u05A5\"]\n",
    "ole_weyoreds_found = 0\n",
    "for au in accent_units:\n",
    "    accent_matches = re.findall(unicode_accent_range, au[\"accent_unit\"])\n",
    "    if accent_matches == ole_weyored:\n",
    "        ole_weyoreds_found += 1\n",
    "        if ole_weyoreds_found < 10:\n",
    "            print(T.sectionFromNode(au[\"nodes\"][0]), au[\"accent_unit\"])\n",
    "        elif ole_weyoreds_found == 10:\n",
    "            print(\"(only printing the first 10)\")\n",
    "print(\"Found altogether:\", ole_weyoreds_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to find the Ole Weyoreds split across two words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Psalms', 1, 3) עַֽל־פַּלְגֵ֫י  מָ֥יִם \n",
      "('Psalms', 4, 7) מִֽי־יַרְאֵ֪נוּ֫  טֹ֥וב \n",
      "('Psalms', 6, 3) אֻמְלַ֫ל  אָ֥נִי \n",
      "('Psalms', 8, 3) יִסַּ֪דְתָּ֫  עֹ֥ז \n",
      "('Psalms', 14, 4) כָּל־פֹּ֪עֲלֵ֫י  אָ֥וֶן \n",
      "('Psalms', 18, 44) מֵרִ֪יבֵ֫י  עָ֥ם \n",
      "('Psalms', 28, 3) וְעִם־פֹּ֪עֲלֵ֫י  אָ֥וֶן \n",
      "('Psalms', 30, 8) לְֽהַרְרִ֫י  עֹ֥ז \n",
      "('Psalms', 31, 19) שִׂפְתֵ֫י  שָׁ֥קֶר \n",
      "('Psalms', 31, 21) מֵֽרֻכְסֵ֫י  אִ֥ישׁ \n",
      "('Psalms', 37, 7) וְהִתְחֹ֪ולֵ֫ל  לֹ֥ו \n",
      "('Psalms', 40, 18) יַחֲשָׁ֫ב  לִ֥י \n",
      "('Psalms', 44, 4) לֹא־הֹושִׁ֪יעָ֫ה  לָּ֥מֹו \n",
      "('Psalms', 45, 8) וַתִּשְׂנָ֫א  רֶ֥שַׁע \n",
      "('Psalms', 53, 3) עַֽל־בְּנֵ֫י  אָדָ֥ם \n",
      "('Psalms', 53, 5) פֹּ֤עֲלֵ֫י  אָ֥וֶן \n",
      "('Psalms', 53, 6) לֹא־הָ֪יָה֫  פָ֥חַד \n",
      "('Psalms', 56, 9) סָפַ֪רְתָּ֫ה  אָ֥תָּה \n",
      "('Psalms', 62, 10) בְּנֵ֫י  אִ֥ישׁ \n",
      "('Psalms', 88, 1) לִבְנֵ֫י  קֹ֥רַח \n",
      "('Psalms', 88, 10) מִנִּ֫י  עֹ֥נִי \n",
      "('Psalms', 97, 10) שִׂנְא֫וּ  רָ֥ע \n",
      "('Psalms', 102, 3) צַ֫ר  לִ֥י \n",
      "('Psalms', 115, 1) לֹ֫א  לָ֥נוּ \n",
      "('Psalms', 130, 7) אֶל־יְה֫וָה  כִּֽי־עִם־יְהוָ֥ה \n",
      "('Psalms', 142, 7) כִּֽי־דַלֹּ֪ותִ֫י  מְאֹ֥ד \n",
      "('Psalms', 144, 2) וּֽמְפַלְטִ֫י  לִ֥י \n",
      "('Proverbs', 1, 22) תְּֽאֵהֲב֫וּ  פֶ֥תִי \n",
      "('Proverbs', 6, 26) עַֽד־כִּכַּ֫ר  לָ֥חֶם \n",
      "('Proverbs', 8, 13) שְֽׂנֹ֫את  רָ֥ע \n",
      "('Proverbs', 8, 34) שֹׁמֵ֪עַֽ֫  לִ֥י \n",
      "('Proverbs', 24, 12) לֹא־יָדַ֪עְנ֫וּ  זֶ֥ה \n",
      "('Proverbs', 25, 7) עֲֽלֵ֫ה  הֵ֥נָּה \n",
      "('Proverbs', 30, 16) וְעֹ֪צֶ֫ר  רָ֥חַם \n",
      "('Proverbs', 30, 19) עֲלֵ֫י  צ֥וּר \n",
      "('Job', 3, 4) יְֽהִ֫י  חֹ֥שֶׁךְ \n",
      "('Job', 3, 6) יִקָּחֵ֪ה֫וּ  אֹ֥פֶל \n",
      "('Job', 7, 11) אֶחֱשָׂ֫ךְ  פִּ֥י \n",
      "('Job', 21, 33) רִגְבֵ֫י  נָ֥חַל \n",
      "('Job', 32, 2) מִמִּשְׁפַּ֪חַ֫ת  רָ֥ם \n",
      "('Job', 33, 9) בְּֽלִ֫י  פָ֥שַׁע \n",
      "('Job', 34, 10) שִׁמְע֫וּ  לִ֥י \n",
      "('Job', 37, 6) הֱוֵ֫א  אָ֥רֶץ \n",
      "('Job', 42, 3) בְּֽלִ֫י  דָ֥עַת \n"
     ]
    }
   ],
   "source": [
    "worked_this_time = 0\n",
    "ole_just_found = False\n",
    "for au in accent_units:\n",
    "    accent_matches = re.findall(unicode_accent_range, au[\"accent_unit\"])\n",
    "    if len(accent_matches):\n",
    "        if accent_matches[-1] == ole_accent:\n",
    "            # Found an ole\n",
    "            ole_just_found = au[\"accent_unit\"]\n",
    "            continue\n",
    "        elif ole_just_found:\n",
    "            if re.search(yored_accent, au[\"accent_unit\"]):\n",
    "                print(T.sectionFromNode(au[\"nodes\"][0]), ole_just_found + \" \" + au[\"accent_unit\"])\n",
    "    ole_just_found = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
